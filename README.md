# Fake-Job-Detector
Classification models that uses text data features and meta-features and predict which job description are fraudulent

The dataset contains 18K job descriptions out of which about 800 are fake. The data consists of both textual information and meta-information about the jobs. Using the textual data about the job postings we created 2 models, namely Logistic Regression and Naive Bayes Classifier which can learn the job descriptions which are fraudulent. We made use of PySpark which is a computational framework to work with Big Data sets. The entire project was done on the Databricks Community Edition Platform to run the Pyspark code. Apache Spark MLlib is one of the many tools that help to bridge this gap by offering most of machine learning models.
Apart from machine learning algorithms, we used Spark MLlib for various feature transformers like Tokenizer, StopWordRemover and features extractors like Hashing TF, TF-IDF, and W. Although these transformers and extractors are sufficient to build basic NLP pipeline but to build a more comprehensive and production-grade pipeline, we made use of more advanced techniques like stemming and lemmatization.
